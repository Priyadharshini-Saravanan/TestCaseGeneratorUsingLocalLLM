# ðŸ“œ Project Constitution â€” Local LLM Test Case Generator

## Discovery Answers
- **North Star:** Local LLM Test Case Generator based on user input with a proper template, using Ollama API (llama3.2)
- **Integrations:** Ollama (local only)
- **Source of Truth:** N/A â€” user provides input directly via chat UI
- **Delivery Payload:** Chat UI where user enters input and sees generated test cases
- **Behavioral Rules:** User enters input â†’ system generates test cases using local LLM via Ollama

---

## Data Schemas

### Input Schema (User â†’ System)
```json
{
  "user_input": "string â€” requirement, user story, or feature description",
  "session_id": "string â€” unique chat session identifier"
}
```

### System Prompt Template (Stored in Code)
```json
{
  "role": "system",
  "content": "Test case generation system prompt with template (to be provided by user)"
}
```

### Output Schema (LLM â†’ User)
```json
{
  "test_cases": [
    {
      "id": "string â€” TC_001",
      "title": "string â€” test case title",
      "description": "string â€” what is being tested",
      "preconditions": "string â€” setup requirements",
      "steps": ["string â€” step 1", "string â€” step 2"],
      "expected_result": "string â€” expected outcome",
      "priority": "string â€” High/Medium/Low"
    }
  ],
  "summary": "string â€” brief summary of generated test cases",
  "total_count": "number â€” total test cases generated"
}
```

---

## Behavioral Rules
1. All LLM calls are **local only** via Ollama â€” no external API calls.
2. Business logic must be **deterministic** â€” LLM is used only for generation.
3. Test case prompt template is **stored in code**, not user-configurable.
4. All intermediate files go in `.tmp/`.
5. Environment config goes in `.env`.
6. Model: **llama3.2** (open source).

---

## Architectural Invariants
1. **3-Layer Architecture (A.N.T.)**:
   - `architecture/` â€” Technical SOPs (Markdown)
   - Navigation â€” Routing/orchestration layer (Python backend)
   - `tools/` â€” Deterministic Python scripts (atomic, testable)
2. If logic changes â†’ Update SOP **before** updating code.
3. No code in `tools/` until Discovery is complete and Data Schema is approved.

---

## Technology Stack
- **LLM Runtime:** Ollama (local)
- **Model:** llama3.2
- **Backend:** Python (Flask)
- **Frontend:** HTML/CSS/JS (Chat UI)
- **Communication:** Ollama Python library (`pip install ollama`)
